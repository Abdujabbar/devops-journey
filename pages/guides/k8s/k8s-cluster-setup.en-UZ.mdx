import { Callout } from "nextra-theme-docs";

## Kubernetes cluster yaratish va sozlash(kubeadm)

> **kubeadm** yordamida **on-premise** servarlarda Kubernetes klasterini yaratish va sozlash bo'yicha to'liq qo'llanma.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/banner.png)

Zamonaviy IT infratuzilmasining dinamik landshaftida konteynerlashtirilgan applicationlarni tartibga solish va boshqarish masshtablilik(scalability), ishonchlilik(reliability) va deployment qulayligini ta'minlashning asosiy jihatiga aylandi. Kubernetes, open-source konteyner orkestrlari platformasi sanoat standarti sifatida paydo bo'lib, tashkilotlarga konteynerlarni boshqarish va orkestrlashni soddalashtirish imkonini beradi.

Ushbu qo'llanma **on-premise**da **kubeadm** yordamida Kubernetes klasterini yaratish va sozlash bo'yicha bosqichma-bosqich qo'llanmani taqdim etishga qaratilgan. **Kubeadm** - bu Kubernetes klasterini yuklash jarayonini soddalashtirish uchun maxsus ishlab chiqilgan tool bo'lib, uni yangi boshlanuvchilar va tajribali mutaxassislar uchun qulay.

System administrator/DevOps Engineer yoki oddiygina konteyner orkestratsiyasi olamini o'rganishni istagan odam bo'lasizmi, ushbu qo'llanma sizni on-premise infratuzilmangizda Kubernetes klasterini o'rnatish jarayoni bo'ylab ko'rsatib beradi. Ushbu qo'llanmaning oxirida siz konteynerlashtirilgan ilovalarni osongina joylashtirish va boshqarishga tayyor ishlaydigan Kubernetes klasteriga ega bo'lasiz.

## Ishni boshlash

Ushbu amaliyotda biz on-primese serverlarda **kubeadm** bilan kubernetes cluster yaratimiz va ishlashga tayyor qilib sozlaymiz. Bu amaliyotda biz HAProxy, MetalLB, NGINX Ingress, flannel, HELM va boshqa texnologiyalarni ishlatamiz.

<Callout type="info" emoji="">

Ushbu amaliyot uchun kerak bo'ladigan serverlar.
**Minimum Server talabi**

| OS            | RAM            | CPU           | Xotira       | Static IP  | Server nomi  |
| ------------- | -------------- | ------------- |------------- | ---------- | -----------  |
| Ubuntu 20.04  | 8GB            | 8vCPU 4 core  | 50GB         | Ha kerak   | k8s-master   |
| Ubuntu 20.04  | 4GB            | 4vCPU 2 core  | 50GB         | shart emas | k8s-node1    |
| Ubuntu 20.04  | 4GB            | 4vCPU 2 core  | 50GB         | shart emas | k8s-node2    |
| Ubuntu 20.04  | 4GB            | 4vCPU 2 core  | 50GB         | shart emas | k8s-node3    |

Umumiy hisobda bizga 4ta server, bitta k8s-master server va minimum uchta server node(k8s-node1,k8s-node2,k8s-node3) kerak bo'ladi.

</Callout>

**k8s-master** serverimizga **static IP** bo'lishi kerak qolgan k8s nodelar esa bitta networkda bitta subnetda bo'lsa shart emas agar har xil network subnetda bo'lishsa static IP kerak bo'ladi.

Quyida meni kubernetes cluster uchun tayyorlab olgan serverlarim ro'yxati. E'tibor berib qarasangiz ularning **Internel IP** manzili **10.28.0.0/24** network subnetda bu degani ushbu serverlar bir biri bilan **Internal IP**si orqali bo'glana oladi. Shuning uchun menga bitta k8s-master sererimda static IP manzil bor. k8s-master serverimga static IP berishimni sababi boshqa serverlar bilan aloqa qilish(masalan Load Balancer) va applicationlar domen ulab **NGINX ingress** bilan expose qilish uchun kerak.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/vm1.png)

## kubeadm bilan k8s cluster yaratish

Amaliyotimiz uchun muhitni serverlarni sozlab olganimizdan keyin amaliyotni boshlasak bo'ladi. Biz kubernetes cluster yataishda **kubeadm** dan foydalanamiz. Ishni ancha osonlashtirish uchun men oldinroq **kubeadm** bilan ishlab kubernetes cluster yaratadigan bash script yozib qo'ygandim. Ushbu amaliyotda biz bash scriptdan foydalanaib k8s cluster yaratamiz. Barcha serverlarimizga ssh orqali kirib olamiz va quyidagicha scriptni ishga tushiramiz.

**K8s installer** script ikki qismga bo'lingan yani k8s-master uchun alohida va k8s-nodelar uchun alhoida. [**K8s installer**](https://github.com/ismoilovdevml/devops-tools/tree/master/k8s/Install) bash scriptni kodlari open-source githubga joylashtirilgan.

**1->** **k8s-master** serverimizfga kirib quyidagi buyruq orqali scriptni ishga tushiramiz.

```bash
curl -sSL https://raw.githubusercontent.com/ismoilovdevml/devops-tools/master/k8s/Install/k8s-master.sh | bash
```

Script quyidagicha ishga tushishi kerak.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s1.png)

Bash script o'z ishini muvaffaqiyatli yakunlagida quyidagicha ko'rinishi kerak.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s3.png)

**2->** k8s-masterda bash script muvvaqiyatli ishini yakunlagida  k8s-node serverlarni ulash uchun **kubeadm join** buryruq generatsiya qilib beradi. E'tiborli bo'lib ushbu buyruqni nusxalab olamiz bu orqali kubernetes node sereverlarni k8s-masterga ulay olamiz.

Buyruq rasmda ko'rsatilgan.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s5.png)

<Callout type="info" emoji="">
```bash
kubeadm join 10.128.0.45:6443 --token snydd0.hdwia5qbrc455aro \
         --discovery-token-ca-cert-hash sha256:ac02bc09d36903d97ceff3e5437b5386dcc5bf25ed90c8ce03dee9ae80061672
```
</Callout>

**3->** k8s nodelar uchun alohida k8s nodelar uchun yozilgan bash scriptni ishga tushiramiz.

```bash
curl -sSL https://raw.githubusercontent.com/ismoilovdevml/devops-tools/master/k8s/Install/k8s-worker.sh | bash
```

Bash script quyidagicha ishga tushishi kerak.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s2.png)

Bash script o'z ishini muvaffaqiyatli yakunlagida quyidagicha ko'rinishi kerak.

![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s4.png)

**4->** Bash scrip k8s-master va barcha nodelarda muvvaqiyatli ishga tushganidan keyin k8s-master da berilgan **kubeadm join** dan foydalani k8s-masterimizga k8s nodelarimizni qo'shib chiqishimiz kerak, ya'ni quyidagi bizga berilgan buyruqni node serverlarimizga **sudo** bilan ishga tushirishiniz kerak.


```bash
sudo kubeadm join 10.128.0.45:6443 --token snydd0.hdwia5qbrc455aro --discovery-token-ca-cert-hash sha256:ac02bc09d36903d97ceff3e5437b5386dcc5bf25ed90c8ce03dee9ae80061672
```
![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s6.png)

Barcha nodelarimizda shu buyruq mauvaqqiyatli bajarilganidan keyin uni tekshirib ko'ramiz.

**5->** K8s node serverlarimiz k8s-masterga bo'glanganini ko'rish uchun k8s-master serverimizga kirib quyidagi buyruq orqali k8s nodelarni ko'ramiz.

```bash
kubectl get nodes
```
![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s7.png)

Bu holatda ulangan nodelkar ko'rsatiladi lekin **STATUS** NotReady bo'lib turibti.

## Network sozlash

Kubernetes klasterini kubeadm yordamida yaratganingizda, klaster dastlab asosiy komponentlar bilan o'rnatiladi, lekin u default bo'yicha sozlangan networking solutioniga ega bo'lmaydi. Networking solutiondagi turli nodelar o'rtasidagi aloqa va ushbu nodelarda ishlaydigan podlar bir-biri bilan aloqa qilish uchun juda muhimdir. Bu holatda, `kubectl get nodes` buyrug'ini ishga tushirganingizda, network solution joyida bo'lmagani uchun nodelar tayyor emas. **NotReady** holati nodelar hali to'liq ishlamaganligini bildiradi.

Biz bu folatda **Flannel** ishlatamiz. 

**1->** Kubernetes clutserimizga flannelni quyidagicha o'rnatamiz.

<Callout type="info" emoji="">
**ESLATMA:** Bundan keyin faqat **k8s-master** serverimizda ishlaymiz.
</Callout>

```bash
kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
```
Flannelni muvaffaqiyatli ishga tushirib `kubectl get nodes` buyru'gi bilan tekshirganimizda barcha nodelarimiz **STATUS**si **Ready** bo'lib turganini ko'rishimiz mumkin.
![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s8.png)

**2->** Kubernetes kalusterimzda qancha podlarimiz ishlab turganini ko'rish uchun quyidagi buyruqdan foydalanamiz.

```bash
kubectl get pods -A
```
![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s9.png )

### MetalLB

**MetalLB** - bare metal Kubernetes klasterlari uchun load balancer. On-primese muhitlarda, ayniqsa bare metal infratuzilmasi bo'lganlarda, servicelar ko'pincha tashqi ko'rinishga ega bo'lish usuliga muhtoj. Boshqariladigan xizmatlar sifatida yuk balanslagichlari mavjud cloud muhitlardan farqli o'laroq, on-premise sozlashlarda bunday infratuzilma bo'lmasligi mumkin.

Kubernetes klasterini bare-metallga o'rnatish uchun **kubeadm** dan foydalanilganda, **LoadBalancer** turidagi servicelar ishlamaydi, chunki tashqi load balanceri mavjud emas. Bu yerda **MetalLB** keladi. **MetalLB** on-primese muhitda yaxshi ishlaydigan dasturiy ta'minotga asoslangan load-balancerni ta'minlaydi. U tashqi trafikni klasterdagi nodelar bo'ylab tarqatadi.

**3->** MetalLB'ni o'rnatishdan oldin kube-proxy Configmapni sozlab olishimiz kerak.


```bash
kubectl get configmap kube-proxy -n kube-system -o yaml | \
sed -e "s/strictARP: false/strictARP: true/" | \
kubectl diff -f - -n kube-system
```

kube-proxy ConfigMap-ni o'zgartirish: **kubectl get configmap kube-proxy** buyruqlari joriy kube-proxy ConfigMapni olish uchun ishlatiladi, so'ngra **sed** **strictARP** qiymatini **false**dan **true**ga o'zgartirish uchun ishlatiladi. Ushbu modifikatsiya MetalLB to'g'ri ishlashi uchun talab qilinadi. Keyin **kubectl apply -f** - buyrug'i o'zgartirilgan ConfigMapni qo'llash uchun ishlatiladi.

```bash
kubectl get configmap kube-proxy -n kube-system -o yaml | \
sed -e "s/strictARP: false/strictARP: true/" | \
kubectl apply -f - -n kube-system
```

**4->** MetalLBdan oldin sozlash yakunlangandan keyin MetalLBni helm orqali o'rnatib olamiz. Bash script ihsga tushirib kuberntes cluter yaratganimizda helm ham o'rnatiladi ya'ni helm o'rntaishga hojat yo'q.

```bash
helm repo add metallb https://metallb.github.io/metallb
helm repo update
helm install metallb metallb/metallb --namespace metallb-system --create-namespace
```

helm metallb repositoriyani qo'shadi va repositoriyalarni yangilab oladi, keyin metallb nomli namespaceda metallbni o'rnatadi va ishga tushiradi. Har bir dastur yoki tool uhcun alohida **namespace** ochib  ishlash Kubernetesda yaxshi amaliyot.

MetalLB ishlab turgani bilish uchun statusini ko'ramiz.

```bash
# barcha namescpaselar ro'yxatini ko'rish uchun
kubectl get ns
# metallb-system namespace podlarini ko'rish

kubectl get pods -n metallb-system
```
![k8s-cluster](/images/tutorials/k8s/cluster-setup/k8s10.png )

**5->** MetalLB o'rnatganimizdan keyin **MetalLB Address Pool** sozlashimiz kerak.

`metallb` nomli jild ochib `address-pool.yaml` fayl ochib yaml configuratsiya yozamiz.

```bash
mkdir metallb
cd metallb
nano address-pool.yaml
```

**MetalLB Address Pool** configuratsiyamiz quyidagicha. [**MetalLB rasmiy qo'llanmasi**](https://metallb.universe.tf/configuration/)

```yaml filename="metallb/address-pool.yaml" {2,5,8,11,16-17}
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.128.0.200-10.128.0.220
---
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  ipAddressPools:
  - first-pool
```

Ushbu konfiguratsiya ikki qismdan iborat `IPAddressPoll` va `L2Advertisement`.

```yaml filename="metallb/address-pool.yaml" {2,4-5,8}
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 10.128.0.200-10.128.0.220
```
* **`apiVersion` vs `kind`** Bu maydonlar API versiyasi va resurs turini belgilaydi. Bu holda, bu **metallb.io/v1beta1** API guruhidagi **IPAddressPool**.

* **metadata ->** Resurs, jumladan uning nomi va namespace haqidagi metama'lumotlarini o'z ichiga oladi.
* **spec ->** Ushbu bo'lim resursning kerakli holatini belgilaydi.
* **addresses ->** **MetalLB LoadBalancer** turidagi servicelarga ajratishi mumkin bo'lgan bir qator IP manzillarni belgilaydi. Bunday holda, u `10.128.0.200` dan `10.128.0.220` gacha bo'lgan diapazondir. **LoadBalancer** turidagi service yaratilganda, **MetalLB** ushbu diapazondan servicega IP belgilaydi(assign).

![k8s-cluster](/images/tutorials/k8s/cluster-setup/ip1.png)

Rasmga e'tibor bersangiz k8s cluster serverlar internal IP si ko'rsatilgan va ular **10.28.0.0/24** network subnetda.

```yaml filename="metallb/address-pool.yaml" {2,7-8}
apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  ipAddressPools:
  - first-pool
```

**ipAddressPools ->** Ushbu **L2Advertiment** resursini oldindan belgilangan **frist-pool** IPAddressPool bilan bog'laydi. Ushbu bog'lanish **MetalLB**ga ko'rsatilgan IP-manzil poolidan **Layer 2**(L2)advertisement uchun foydalanish kerakligi haqida xabar beradi.

Konfiguratsiya asosan IP-manzillar poolini (IPAddressPool) e'lon qiladi, undan MetalLB LoadBalancer tipidagi servicelar uchun manzillarni ajratishi mumkin. Shuningdek, u tarmoq qurilmalariga ko'rsatilgan IP manzillar poolidan foydalanish mumkinligi haqida xabar berish uchun Layer 2 advertisement (L2Advertisement) o'rnatadi. Bu, ayniqsa, IP-manzilni aniqlash uchun **ARP** (Address Resolution Protocol) ishlatiladigan muhitlarda foydalidir.

Ushbu konfiguratsiya MetalLB-ga tashqi IP-manzillarni LoadBalancer tipidagi servicelarga ajratish uchun belgilangan IP-manzil oralig'idan foydalanishni buyuradi va bu ma'lumotni tarmoqning to'g'ri rezolyutsiyasi uchun Layer 2 advertisement orqali yetkazadi. Bu tashqi trafikning Kubernetes klasteringizdagi tegishli podlarga yetib borishiga imkon berish uchun juda muhimdir.

**6->** MetalLB konfiguratsiya faylimizni yozib bo'lganimzdan keyin uni apply qilib ishga tushiramiz.

```bash
kubectl apply -f address-pool.yaml
```

### NGINX Ingress Controller

**Nginx Ingress Controller** Kubernetes resource va controlleri bo'lib, Kubernetes klasteridagi servicelarga tashqi kirishni boshqaradi. U aqlli **HTTP** va **HTTPS** trafik menejeri sifatida ishlaydi. NGINX Ingress Controller Kubernetes klasteridagi servicelarga tashqi kirishni boshqaradi va
routing, load-balancing, SSL termination va boshqalarni boshqaradi.


**1->** NGINX Ingress Controllerni helm orqalim o'rnatib olamiz.

```bash 
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update
helm install ingress-nginx ingress-nginx/ingress-nginx --namespace ingress-nginx --create-namespace
```

Statusni ko'rish uchun

```bash
kubectl get svc -n ingress-nginx
kubectl get pods -n ingress-nginx
```

![k8s-cluster](/images/tutorials/k8s/cluster-setup/ip2.png)

**EXTERNAL IP** sifatida MetalLB orqali bergan address pooldan 10.128.0.200 IPni oldi.

### HAProxy
Tashqaridan kiruvchi tarfikni TCP orqali kubernetes cluster ichidagi ingresga yo'naltirish uchun foydalanamiz.

**1->** HAProxyni k8s-matser serverga o'rnatib olamiz.
```bash
sudo apt update && sudo apt upgrade -y
sudo apt install haproxy -y
sudo systemctl enable haproxy
sudo systemctl status haproxy
```
**2->** HAProxyni o'rnatib olagnimizdan keyin `/etc/haproxy/haproxy.cfg` ni ochamiz. `haproxy.cfg` konfiguratsiya fayli dastlab default holda quyidagicha bo'ladi.
```bash filename="/etc/haproxy/haproxy.cfg"
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
    ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
    ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
    ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
	log	global
	mode	http
	option	httplog
	option	dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http
```

**3->* *HAProxyni quyidagicha konfiguratsiya qilib NGINX Ingress Controllerimizni ulaymiz.


```bash filename="/etc/haproxy/haproxy.cfg" {22-23,37-40,42-44,46-49,51-53}
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats socket /run/haproxy/admin.sock mode 660 level admin expose-fd listeners
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

    # Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# See: https://ssl-config.mozilla.org/#server=haproxy&server-version=2.0.3&config=intermediate
    ssl-default-bind-ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384
    ssl-default-bind-ciphersuites TLS_AES_128_GCM_SHA256:TLS_AES_256_GCM_SHA384:TLS_CHACHA20_POLY1305_SHA256
    ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
	log	global
	mode	tcp
#	option	httplog
	option	dontlognull
    timeout connect 5000
    timeout client  50000
    timeout server  50000
	errorfile 400 /etc/haproxy/errors/400.http
	errorfile 403 /etc/haproxy/errors/403.http
	errorfile 408 /etc/haproxy/errors/408.http
	errorfile 500 /etc/haproxy/errors/500.http
	errorfile 502 /etc/haproxy/errors/502.http
	errorfile 503 /etc/haproxy/errors/503.http
	errorfile 504 /etc/haproxy/errors/504.http


frontend ingress
   mode tcp
   bind :80
   default_backend ingress_servers

backend ingress_servers
   mode tcp
   server s1 10.128.0.200:80 check

frontend ingress_tls
   mode tcp
   bind :443
   default_backend ingress_servers_tls

backend ingress_servers_tls
   mode tcp
   server s2 10.128.0.200:443 check
```

Qisqa qilib aytganda tashqaridan kirishlar HAProxyda TCP mode orqali kubernetes clusterimizdagi NGINX Ingress Controllerga kiradi. HAProxyga yuqorida ko'rsatilgan NGINX Ingress Controller **EXTERNAL IP**si berilgan yani **10.128.0.200**

**4->** haproxy.cfg configuratsiyamizni tekshirib olamiz va haproxyga restart berib ishga tushiramiz.

```bash
haproxy -c -f /etc/haproxy/haproxy.cfg
sudo systemctl restart haproxy
sudo systemctl status haproxy
```

## Kubernetesga Cert-Manager o'rnatish va sozlash

