import { Callout } from "nextra-theme-docs";

# Patroni va HAProxy yordamida Highly Available PostgreSQL clusterini yaratish

![postgres-cluster](/images/tutorials/database/postgres-cluster/banner.png)

## Kirish

Bugungi raqamli landshaftda ma'lumotlar bazalarining high availability va ishonchliligini ta'minlash barcha o'lchamdagi korxonalar va tashkilotlar uchun muhim ahamiyatga ega. O'zining mustahkamligi, scalability va extensibility bilan mashhur bo'lgan PostgreSQL ko'plab muhim application uchun asos bo'lib xizmat qiladi. PostgreSQL ma'lumotlar bazalarining ishlash muddati va barqarorligini oshirish uchun highly available bo'lgan klaster arxitekturasini o'rnatishga ehtiyoj seziladi.

Ushbu texnik qo'llanma sizga **Patroni** va **HAProxy**-dan foydalangan holda highly available bo'lgan PostgreSQL klasterini yaratish jarayonini o'tkazishga qaratilgan. Patroni, PostgreSQL uchun high-leveldagi Python orkestratori postgreSQL klasterlarini o'zgartirish va replikatsiya kabi vazifalarni avtomatlashtirish orqali boshqarishni soddalashtiradi. HAProxy, ko'p qirrali va kuchli load balancer, clientlar ulanishlarini bir nechta PostgreSQL misollari bo'ylab taqsimlashni osonlashtiradi, resurslardan samarali foydalanish va uzluksiz uzilishni ta'minlaydi.

Ushbu qo'llanmaning oxirida siz bardoshli PostgreSQL klaster arxitekturasini qanday loyihalash, amalga oshirish va qo'llab-quvvatlash haqida  tushunchaga ega bo'lasiz, bu sizga ma'lumotlar bazasi infratuzilmangizda yuqori darajadagi mavjudlik(high availability) va xatolarga chidamlilik(tolerance) talablarini qondirish imkonini beradi. Keling, potentsial nosozliklarga chidamli va qimmatli ma'lumotlaringizga uzluksiz kirishni ta'minlaydigan mustahkam PostgreSQL klasterini yaratish yo'lida ushbu sayohatni boshlaylik.

## Ishni Boshlash

Ushbu amaliyotda biz **Ubuntu 20.04** serverlarda patroni, etcd, pgbouncer va HAProxy yordamida PostgreSQL cluster yaratamiz. 

**Patroni** - bu PostgreSQL klasterlarini boshqarishni avtomatlashtirish uchun mo'ljallangan open-source tool bo'lib, high availability va barqarorlikni ta'minlashga qaratilgan. U PostgreSQL instancelarini bir nechta nodelar bo'ylab joylashtirish va ularga xizmat ko'rsatishni, ularni doimiy ravishda  helath check qilib kuzatib borish va nosozliklar yoki jiddiy muammolar yuzaga kelganda avtomatlashtirilgan uzilish protseduralarini tartibga solish orqali soddalashtiradi.

Patronining asosiy xususiyatlari quyidagilardan iborat: avtomatlashtirilgan uzilish, taqsimlangan consensus algoritmlaridan foydalangan holda leaderni tanlash, markazlashtirilgan konfiguratsiyani boshqarish, **pgBouncer** va **HAProxy** kabi ekotizim toollari bilan uzluksiz integratsiya va sozlash uchun kengaytirilganlik.

**PgBouncer** - bu PostgreSQL ma'lumotlar bazalari uchun lightweight connection pooler. U client applicationlari va PostgreSQL ma'lumotlar bazasi serverlari o'rtasida joylashgan bo'lib, ma'lumotlar bazasi ulanishlarini boshqaradigan va optimallashtiradigan vositachi sifatida ishlaydi.U ma'lumotlar bazasi arxitekturalarida, ayniqsa ulanishning uzilishi yoki talab qilinadigan ish yuklari bo'lgan muhitlarda muhim komponent bo'lib xizmat qiladi, ma'lumotlar bazasi resurslaridan samarali foydalanish va client applicationlari uchun javob berish qobiliyatini oshiradi.

**etcd** - yuqori ishonchli distributed systemlarni yaratish uchun mo'ljallangan distributed key-value store. CoreOS tomonidan ishlab chiqilgan bo'lib, u barqarorlik va nosozliklarga chidamliligini ta'minlagan holda ma'lumotlarni machinelar klasterida saqlashning ishonchli usulini ta'minlaydi. Oddiy API va mustahkam dizayni bilan etcd distributed applicationlarni ishlab chiqishni soddalashtiradi va dinamik muhitda muammosiz masshtablash va moslashuvchanlikni ta'minlaydi.

<Callout type="info" emoji="">
Ushbu amaliyotda biz production uchun yuqorida darajada yozilgan [github.com/vitabaks/postgresql_cluster](https://github.com/vitabaks/postgresql_cluster) **Ansible** playbookdan foydalanamiz. Ushbu amaliyot uchun moslangan versiyasi [github.com/devops-journey-uz/postgresql_cluster](https://github.com/devops-journey-uz/postgresql_cluster)

Ushbu Ansible playbook qo'llab quvvvatlaydigan serverlar
* **Debian:** 10, 11, 12
* **Ubuntu:** 18.04, 20.04, 22.04
* **CentOS:** 7, 8
* **CentOS Stream:** 8, 9
* **Oracle Linux:** 7, 8, 9
* **Rocky Linux:** 8, 9
* **AlmaLinux:** 8, 9
</Callout>

## Arxitekura

Ushbu amaliyotda quyidagi rasmdagi arxitekturani quramiz. Bizda bitta **etcd** va bitta **load-balancer** serverimiz bo'ladi. Ikkita postgres replica va bitta masterimiz bo'ladi.

![postgres-cluster](/images/tutorials/database/postgres-cluster/architecture.png)


Quyidagi rasmda esa network arxitekturamiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/network-architecture.png)

Bizda Postgresql clusterimiz uchun **db-network**kimiz bor uni ichida esa **db-subnet** nomli **172.20.0.0/16** IP rangelik subnetimiz bor. Barcha serverlarimiz bir biriga internal IP bilan bo'glanib ishlaydi, bitta load-balancer serverimizda static(public) IP address bo'ladi holos. Agar siz Azure ishlatsangiz [Azureda Network sozlash](https://devops-journey.uz/guides/cloud/azure-network-sozlash) amaliyotidan quyidagi networkni qurib olishingiz mumkin.

## Environment sozlash

Ushbu amaliyotni cloud va on-primise environmentda qilishimiz mumkin. Buning uchun bizga bitta network va subnet kerak bo'ladi serverlarimiz internal IP bilan bog'lanishi uchun. Minimum 6ta server kerak bo'ladi.

Ushbu amaliyot uchun ishlatilgan server IP manzillarimiz namuna hisoblanadi qo'llanmada chalg'imasligingiz uchun.

| RAM            | CPU           | Xotira       | Internal IP  |  Server Nomi         |
| -------------- | ------------- |------------- | ------------ | -------------------- |
| 8GB            | 2vCPU 2 core  | 100GB        | 170.20.0.4   | etcd                 |
| 8GB            | 2vCPU 2 core  | 50GB         | 170.20.0.5   | load-balancer        |
| 8GB            | 2vCPU 2 core  | 50GB         | 170.20.0.6   | node1                |
| 8GB            | 2vCPU 2 core  | 50GB         | 170.20.0.7   | node2                |
| 8GB            | 2vCPU 2 core  | 50GB         | 170.20.0.8   | node3                |
| 4GB            | 2vCPU 2 core  | 50GB         | 170.20.0.9   | ansible              |


## Network sozlash

Ushbu amaliyotda birinchi qiladigan ishimiz bu network sozlashdir keling ishni network sozlashdan boshlaymiz.

Siz quyidagidek network sozlab olishingiz kerak bo'ladi barcha serverlar bitta networkda bitta subnetda bo'lishi va bir bir bilan internal IP bilan bog'lanib ishlashi kerak faqat load-balancer serverimizda static(public) IP address bo'ladi ya'ni tashqaridan ulana olish uchun. Ushnu network arxitektura PostgreSQL clusterimizni kengaytirishda ham ko'p qulaylik beradi.

![postgres-cluster](/images/tutorials/database/postgres-cluster/network-architecture.png)

Ushbu network arxitekturani on-primise va cloudda ham sozlashingiz mumkin. Agar siz **Microsoft Azure** ishlatsangiz [Azure Network sozlash](https://devops-journey.uz/guides/cloud/azure-network-sozlash) qo'llanmaisdan quyidagi networkni sozlab olishingiz mumkin.

## Ansible server sozlash

Network sozlab olganimizdan keyin shu networkda bitta ansible server yaratib olamiz. Barcha ishni shu ansible server orqali bajaramiz va ansible server boshqa serverlarga internal IP bilan ulana olishi kerak.

Ansible serverimizga ansible o'rnatimiz olamiz.

```bash
sudo apt update && sudo apt upgrade -y
```

```bash
sudo apt-get install software-properties-common python3-pip sshpass git
sudo apt-add-repository ppa:ansible/ansible
```
```bash
sudo apt-get update
sudo apt-get install ansible
```

Ansible o'rnatib olganimizdan keyin Ansible serverimizda ssh key generatsiya qilib barcha serverlarga root user bilann ssh orqali ulanishini sozlab chiqishimiz kerak. Buning uchun ansible servermizda ssh key generatsiya qilamiz va public keyni ansible serverimiz authorized_keys'gan qo'shib qo'yamiz. Ansible serverimizdagi public ssh keyimizni esa barcha serverlarda root userga kirib authorized_keys'ga qo'shib chiqamiz va har bir serverga root userdan ansible serverdan ulanib ko'ramiz.

**1->** Ansible serverdan

```bash
ssh-keygen -f ~/.ssh/db-cluster
cd ~/.ssh/
cat db-cluster.pub >> authorized_keys
```

Ansible serverdagi **db-cluster.pub** keyni nusxalab olamiz.

```bash
cd ~/.ssh/
cat db-cluster.pub
```

**2->** Boshqa serverlarda. Ansible boshqa serverlarga root user bilan kira olishi kerak. barcha serverlarda **db-cluster.pub** keyimizni serverlar authorized_keys'ga qo'shib chiqganimzidan keyin internal ip bilan ssh orqali bog'lanishni tekshirib ko'ramiz.

```bash
sudo su
```
```bash
cat >> ~/.ssh/authorized_keys <<EOF
ssh-rsa ssh-keyimizni joylashtiramiz
EOF
```

## Cluster yaratish

Ansible serverimizni sozlab boshqa sereverlar bilan ssh ulanishni bog'laganimizdan keyin Postgresql cluster yaratishni boshlasak bo'ladi. [github.com/vitabaks/postgresql_cluster](https://github.com/vitabaks/postgresql_cluster) ansible playbookni ansible serverimizda git clone qilib olamiz va o'zimizga moslab configuratsiya qilib olamiz.

**1->** Ansible serverimizda ansible playbookni git clone qilib olamiz.

```bash
git clone https://github.com/vitabaks/postgresql_cluster.git
cd postgresql_cluster/
```

**2->** `inventory` konfiguratsiya fayliga serverlarimiz internal IP addrslarini qo'shib chiqamiz.

| server         | Internal IP    |
| -------------- | -------------- |
| etcd           | 170.20.0.4     |
| load-balancer  | 170.20.0.5     |
| node1          | 170.20.0.6     |
| node2          | 170.20.0.7     |
| node3          | 170.20.0.8     |


![postgres-cluster](/images/tutorials/database/postgres-cluster/inventory.png)

**3->** `postgresql_cluster/vars/system.yml` konfiguratsiya faylidan timezoneni belgilaymiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/timezone.png)

**4->** `postgresql_cluster/vars/main.yml` konfiguratsiya faylidan postgres pgbouncer parollarnini belgilaymiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/password.png)

![postgres-cluster](/images/tutorials/database/postgres-cluster/pgbouncer1.png)

**5->** Barchasini o'zimizga moslab sozlab olganimizdan keyin barcha serverlarga ulanish bor yoki yo'qligini tekshiramiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/ping.png)

**6->** Yuqoridagidek barcha serverlarga ansible serverimiz muvaffaqiyatli ulana olagidan keyin **deploy_pgcluster.yml** orqali TimescaleDB bilan  postgres clusterni yaratishni boshlaymiz. Bu serverlar holati va internet tezligi qaran vaqt oladi.

**->** [**Timescale PostgreSQL ++**](https://www.timescale.com/)

```bash
ansible-playbook deploy_pgcluster.yml -e "enable_timescale=true"
```

**7->** Jarayon muvaffaqiyatli tugaganida quyidagicha ko'rinishda bo'ladi.

![postgres-cluster](/images/tutorials/database/postgres-cluster/ansible1.png)
![postgres-cluster](/images/tutorials/database/postgres-cluster/ansible2.png)

Okey bizda hammasi muvaffaqiyatli yakunlandi keling endi etcd, patroni va pgbouncer statusini va konfiguratsiyalarini ko'rib chiqamiz.

etcd serverimizda etcd muvaffaqiyatli o'rnatilib konfiguratsiya qilingan va holatyi aktiv ishlamoqda.
![postgres-cluster](/images/tutorials/database/postgres-cluster/ansible3.png)

![postgres-cluster](/images/tutorials/database/postgres-cluster/etcd.png)

birorta node1,node2,node3 serverlimizga kirib patroni orqali Postgresql clusterimiz statusini ko'ramiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/patroni.png)

Patroni statusni tekshiramiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/patroni2.png)

Patroni konfiguratsiya(node3).

```bash
---

scope: postgres-cluster
name: pgnode03
namespace: /service


restapi:
  listen: 170.20.0.8:8008
  connect_address: 170.20.0.8:8008
#  certfile: /etc/ssl/certs/ssl-cert-snakeoil.pem
#  keyfile: /etc/ssl/private/ssl-cert-snakeoil.key
#  authentication:
#    username: username
#    password: password

etcd3:
  hosts: 170.20.0.4:2379

bootstrap:
  method: initdb
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576
    master_start_timeout: 300
    synchronous_mode: false
    synchronous_mode_strict: false
    synchronous_node_count: 1
    postgresql:
      use_pg_rewind: true
      use_slots: true
      parameters:
        max_connections: 500
        superuser_reserved_connections: 5
        password_encryption: scram-sha-256
        max_locks_per_transaction: 512
        max_prepared_transactions: 0
        huge_pages: try
        shared_buffers: 1972MB
        effective_cache_size: 5916MB
        work_mem: 128MB
        maintenance_work_mem: 256MB
        checkpoint_timeout: 15min
        checkpoint_completion_target: 0.9
        min_wal_size: 2GB
        max_wal_size: 8GB
        wal_buffers: 32MB
        default_statistics_target: 1000
        seq_page_cost: 1
        random_page_cost: 1.1
        effective_io_concurrency: 200
        synchronous_commit: on
        autovacuum: on
        autovacuum_max_workers: 5
        autovacuum_vacuum_scale_factor: 0.01
        autovacuum_analyze_scale_factor: 0.01
        autovacuum_vacuum_cost_limit: 500
        autovacuum_vacuum_cost_delay: 2
        autovacuum_naptime: 1s
        max_files_per_process: 4096
        archive_mode: on
        archive_timeout: 1800s
        archive_command: cd .
        wal_level: replica
        wal_keep_size: 2GB
        max_wal_senders: 10
        max_replication_slots: 10
        hot_standby: on
        wal_log_hints: on
        wal_compression: on
        pg_stat_statements.max: 10000
        pg_stat_statements.track: all
        pg_stat_statements.track_utility: false
        pg_stat_statements.save: true
        auto_explain.log_min_duration: 10s
        auto_explain.log_analyze: true
        auto_explain.log_buffers: true
        auto_explain.log_timing: false
        auto_explain.log_triggers: true
        auto_explain.log_verbose: true
        auto_explain.log_nested_statements: true
        auto_explain.sample_rate: 0.01
        track_io_timing: on
        log_lock_waits: on
        log_temp_files: 0
        track_activities: on
        track_activity_query_size: 4096
        track_counts: on
        track_functions: all
        log_checkpoints: on
        logging_collector: on
        log_truncate_on_rotation: on
        log_rotation_age: 1d
        log_rotation_size: 0
        log_line_prefix: '%t [%p-%l] %r %q%u@%d '
        log_filename: postgresql-%a.log
        log_directory: /var/log/postgresql
        hot_standby_feedback: on
        max_standby_streaming_delay: 30s
        wal_receiver_status_interval: 10s
        idle_in_transaction_session_timeout: 10min
        jit: off
        max_worker_processes: 24
        max_parallel_workers: 8
        max_parallel_workers_per_gather: 2
        max_parallel_maintenance_workers: 2
        tcp_keepalives_count: 10
        tcp_keepalives_idle: 300
        tcp_keepalives_interval: 30
        shared_preload_libraries: pg_stat_statements,auto_explain,timescaledb

  initdb:  # List options to be passed on to initdb
    - encoding: UTF8
    - locale: en_US.UTF-8
    - data-checksums

  pg_hba:  # Add following lines to pg_hba.conf after running 'initdb'
    - host replication replicator 127.0.0.1/32 scram-sha-256
    - host all all 0.0.0.0/0 scram-sha-256


postgresql:
  listen: 170.20.0.8,127.0.0.1:5432
  connect_address: 170.20.0.8:5432
  use_unix_socket: true
  data_dir: /var/lib/postgresql/16/main
  bin_dir: /usr/lib/postgresql/16/bin
  config_dir: /etc/postgresql/16/main
  pgpass: /var/lib/postgresql/.pgpass_patroni
  authentication:
    replication:
      username: replicator
      password: replicator-parol-IIWfghweQHRW32
    superuser:
      username: postgres
      password: admin-parol-qvcnwepOYUI45Wds
#    rewind:  # Has no effect on postgres 10 and lower
#      username: rewind_user
#      password: rewind_password
  parameters:
    unix_socket_directories: /var/run/postgresql


  remove_data_directory_on_rewind_failure: false
  remove_data_directory_on_diverged_timelines: false


  create_replica_methods:
    - basebackup
  basebackup:
    max-rate: '100M'
    checkpoint: 'fast'


watchdog:
  mode: automatic  # Allowed values: off, automatic, required
  device: /dev/watchdog  # Path to the watchdog device
  safety_margin: 5

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false

  # specify a node to replicate from (cascading replication)
#  replicatefrom: (node name)
```

Okey bizda patroni muvaffaqiyatli ishga tushirilib production uchun konfiguratsiya qilingan.

PGBouncer statusini tekshiramiz.

![postgres-cluster](/images/tutorials/database/postgres-cluster/pgbouncer.png)

`/etc/pgbouncer/pgbouncer.ini` konfiguratsiya

```bash
[databases]
postgres = host=/var/run/postgresql port=5432 dbname=postgres 

* = host=/var/run/postgresql port=5432

[pgbouncer]
logfile = /var/log/pgbouncer/pgbouncer.log
pidfile = /run/pgbouncer/pgbouncer.pid
listen_addr = 0.0.0.0
listen_port = 6432
unix_socket_dir = /var/run/pgbouncer
auth_type = scram-sha-256
auth_user = pgbouncer
auth_dbname = postgres
auth_query = SELECT usename, passwd FROM user_search($1)
admin_users = postgres
stats_users = postgres
ignore_startup_parameters = extra_float_digits,geqo,search_path

pool_mode = session
server_reset_query = DISCARD ALL
max_client_conn = 10000
default_pool_size = 20
query_wait_timeout = 120
reserve_pool_size = 1
reserve_pool_timeout = 1
max_db_connections = 1000
pkt_buf = 8192
listen_backlog = 4096
max_prepared_statements = 1024
so_reuseport = 1

log_connections = 0
log_disconnections = 0

# Documentation https://pgbouncer.github.io/config.html
```

Okeey hammasi zo'r hammasi muvaffaqiyatli ishga tushdi va ishlab turibti endi master va replicalar uchun HAProxy load balancer sozlashimiz kerak.

## HAProxy sozlash